{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMFHtLx+eLe8MWOBHbbKs0i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Faraz1306/prog_multimedia_S5/blob/tp4/TP4_multim%C3%A9dia.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1:\n",
        "\n"
      ],
      "metadata": {
        "id": "W_PsFQM9yCLU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeAmSdMjx_L1",
        "outputId": "a28f9f9a-fefc-4b52-9b86-c9043986646a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Erreur : Impossible d'ouvrir la webcam\n",
            "L'enregistrement vidéo commence...\n",
            "Enregistrement terminé.\n",
            "Ma Webcam ne fonctionne pas\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "\n",
        "# Initialisation de la capture vidéo (webcam)\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "# Vérifiez si la webcam est accessible\n",
        "if not cap.isOpened():\n",
        "    print(\"Erreur : Impossible d'ouvrir la webcam\")\n",
        "    exit()\n",
        "\n",
        "# Définir la résolution de la vidéo (facultatif)\n",
        "frame_width = int(cap.get(3))  # Largeur de la vidéo\n",
        "frame_height = int(cap.get(4))  # Hauteur de la vidéo\n",
        "output_size = (frame_width, frame_height)\n",
        "\n",
        "# Définir le codec et créer l'objet VideoWriter pour enregistrer la vidéo\n",
        "fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Codec vidéo (XVID pour .avi)\n",
        "out = cv2.VideoWriter('video_enregistree.avi', fourcc, 20.0, output_size)\n",
        "\n",
        "# Temps d'enregistrement en secondes\n",
        "record_time = 5  # Par exemple, 10 secondes\n",
        "\n",
        "# Initialiser la durée d'enregistrement\n",
        "fps = 20  # Frame per second\n",
        "frame_count = 0\n",
        "total_frames = record_time * fps\n",
        "\n",
        "print(\"L'enregistrement vidéo commence...\")\n",
        "\n",
        "while frame_count < total_frames:\n",
        "    ret, frame = cap.read()  # Lire une image de la webcam\n",
        "    if ret:\n",
        "        out.write(frame)  # Enregistrer l'image dans le fichier vidéo\n",
        "        cv2.imshow('Enregistrement en cours', frame)  # Afficher l'image dans une fenêtre\n",
        "        if cv2.waitKey(1) & 0xFF == ord('q'):  # Appuyer sur 'q' pour quitter\n",
        "            break\n",
        "        frame_count += 1\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Libérer les ressources\n",
        "cap.release()\n",
        "out.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "print(\"Enregistrement terminé.\")\n",
        "print(\"Ma Webcam ne fonctionne pas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2:"
      ],
      "metadata": {
        "id": "Qgeatkt7z210"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def integrer_image_dans_video(video_source, fichier_sortie, image_path, start_frame, end_frame):\n",
        "    \"\"\"\n",
        "    Intègre une image dans une vidéo existante à un moment donné (sur plusieurs frames).\n",
        "\n",
        "    Args:\n",
        "    - video_source (str) : Le fichier vidéo source.\n",
        "    - fichier_sortie (str) : Le fichier de sortie de la vidéo modifiée.\n",
        "    - image_path (str) : Le chemin vers l'image à incruster.\n",
        "    - start_frame (int) : Frame où l'image commence à apparaître.\n",
        "    - end_frame (int) : Frame où l'image cesse d'apparaître.\n",
        "    \"\"\"\n",
        "    # Charger la vidéo\n",
        "    cap = cv2.VideoCapture(video_source)\n",
        "\n",
        "    # Charger l'image à incruster\n",
        "    image_overlay = cv2.imread(image_path)\n",
        "\n",
        "    # Vérifier que l'image a bien été chargée\n",
        "    if image_overlay is None:\n",
        "        print(\"Erreur : Impossible de charger l'image.\")\n",
        "        return\n",
        "\n",
        "    # Obtenir les propriétés de la vidéo\n",
        "    frame_width = int(cap.get(3))\n",
        "    frame_height = int(cap.get(4))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Redimensionner l'image pour qu'elle ait la même taille que la vidéo\n",
        "    image_overlay = cv2.resize(image_overlay, (frame_width, frame_height))\n",
        "\n",
        "    # Définir le codec et l'objet VideoWriter pour enregistrer la vidéo modifiée\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    out = cv2.VideoWriter(fichier_sortie, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    # Compter les frames\n",
        "    frame_count = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Incruster l'image entre les frames spécifiées\n",
        "        if start_frame <= frame_count < end_frame:\n",
        "            alpha = 0.3  # Transparence de l'image\n",
        "            frame = cv2.addWeighted(frame, 1 - alpha, image_overlay, alpha, 0)\n",
        "\n",
        "        # Enregistrer la frame modifiée\n",
        "        out.write(frame)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    # Libérer les ressources\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    print(f\"L'image a été incrustée entre les frames {start_frame} et {end_frame} et sauvegardée sous '{fichier_sortie}'.\")\n",
        "\n",
        "# Exemple pour intégrer une image dans une vidéo déjà existante\n",
        "\n",
        "fichier_source = 'video_multimedia.mp4'  # Nom du fichier de la vidéo source\n",
        "fichier_sortie = 'video_2.mp4'  # Nom du fichier de la vidéo modifiée avec l'incrustation\n",
        "image_a_incruster = 'logo_iut_montreuil.png'  # Chemin vers l'image à incruster\n",
        "\n",
        "# Incruster l'image entre les frames 50 et 100\n",
        "integrer_image_dans_video(fichier_source, fichier_sortie, image_a_incruster, start_frame=50, end_frame=51)\n",
        "\n",
        "# Ce code va :\n",
        "# - Charger la vidéo 'video_multimedia.mp4'.\n",
        "# - Incruster l'image 'logo_iut_montreuil.png' sur les frames entre 50 et 100.\n",
        "# - Sauvegarder la vidéo modifiée sous 'video_2.mp4'.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXJw15bez6f0",
        "outputId": "94326e5a-af65-4cc5-a9ff-58257fdb24f9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L'image a été incrustée entre les frames 50 et 51 et sauvegardée sous 'video_2.mp4'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3:"
      ],
      "metadata": {
        "id": "pwdrUracDNEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def ajouter_infos_a_video(video_source, fichier_sortie, nom, prenom):\n",
        "    \"\"\"\n",
        "    Ajoute des informations à chaque image d'une vidéo existante.\n",
        "\n",
        "    Args:\n",
        "    - video_source (str) : Le fichier vidéo source.\n",
        "    - fichier_sortie (str) : Le fichier de sortie de la vidéo modifiée.\n",
        "    - nom (str) : Nom de l'utilisateur.\n",
        "    - prenom (str) : Prénom de l'utilisateur.\n",
        "    \"\"\"\n",
        "    # Charger la vidéo\n",
        "    cap = cv2.VideoCapture(video_source)\n",
        "\n",
        "    # Obtenir les propriétés de la vidéo\n",
        "    frame_width = int(cap.get(3))\n",
        "    frame_height = int(cap.get(4))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Définir le codec et l'objet VideoWriter pour enregistrer la vidéo modifiée\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    out = cv2.VideoWriter(fichier_sortie, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    frame_count = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Calculer le timestamp en millisecondes\n",
        "        timestamp_ms = frame_count * 1000 / fps\n",
        "\n",
        "        # Ajouter le nom et prénom au centre en bas de la vidéo\n",
        "        cv2.putText(frame, f\"{nom} {prenom}\",\n",
        "                    (frame_width // 2 - 100, frame_height - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    0.75,\n",
        "                    (255, 255, 255),\n",
        "                    2,\n",
        "                    cv2.LINE_AA)\n",
        "\n",
        "        # Ajouter le timestamp en haut à gauche\n",
        "        cv2.putText(frame, f\"Timestamp: {int(timestamp_ms)} ms\",\n",
        "                    (10, 30),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    0.75,\n",
        "                    (255, 255, 255),\n",
        "                    2,\n",
        "                    cv2.LINE_AA)\n",
        "\n",
        "        # Ajouter l'indice de l'image actuelle sur le nombre de FPS en haut à droite\n",
        "        cv2.putText(frame, f\"{frame_count + 1}/{total_frames}\",\n",
        "                    (frame_width - 150, 30),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    0.75,\n",
        "                    (255, 255, 255),\n",
        "                    2,\n",
        "                    cv2.LINE_AA)\n",
        "\n",
        "        # Enregistrer la frame modifiée\n",
        "        out.write(frame)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    # Libérer les ressources\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    print(f\"Les informations ont été ajoutées à la vidéo et sauvegardées sous '{fichier_sortie}'.\")\n",
        "\n",
        "# Exemple pour ajouter des informations à une vidéo déjà existante\n",
        "\n",
        "fichier_source = 'video_2.mp4'  # Nom du fichier de la vidéo source\n",
        "fichier_sortie = 'video_avec_infos.mp4'  # Nom du fichier de la vidéo modifiée\n",
        "nom = 'Siddiqui'  # Ton nom\n",
        "prenom = 'Faraz'  # Ton prénom\n",
        "\n",
        "# Ajouter des informations à la vidéo\n",
        "ajouter_infos_a_video(fichier_source, fichier_sortie, nom, prenom)\n",
        "\n",
        "# Ce code va :\n",
        "# - Charger la vidéo 'video_2.mp4'.\n",
        "# - Ajouter ton nom, le timestamp et l'indice de l'image à chaque frame.\n",
        "# - Sauvegarder la vidéo modifiée sous 'video_avec_infos.mp4'.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pxRs--ZjDOld",
        "outputId": "6309dbc0-62ec-4eb9-91fd-4d42d372784a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Les informations ont été ajoutées à la vidéo et sauvegardées sous 'video_avec_infos.mp4'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4:"
      ],
      "metadata": {
        "id": "SXbuBiqeD1ow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def appliquer_filtre_sobel(video_source, fichier_sortie):\n",
        "    \"\"\"\n",
        "    Applique un filtre de Sobel à chaque image d'une vidéo existante.\n",
        "\n",
        "    Args:\n",
        "    - video_source (str) : Le fichier vidéo source.\n",
        "    - fichier_sortie (str) : Le fichier de sortie de la vidéo modifiée.\n",
        "    \"\"\"\n",
        "    # Charger la vidéo\n",
        "    cap = cv2.VideoCapture(video_source)\n",
        "\n",
        "    # Obtenir les propriétés de la vidéo\n",
        "    frame_width = int(cap.get(3))\n",
        "    frame_height = int(cap.get(4))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Définir le codec et l'objet VideoWriter pour enregistrer la vidéo modifiée\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    out = cv2.VideoWriter(fichier_sortie, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Appliquer le filtre de Sobel\n",
        "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convertir en niveaux de gris\n",
        "        sobel_x = cv2.Sobel(gray_frame, cv2.CV_64F, 1, 0, ksize=5)  # Filtre de Sobel en X\n",
        "        sobel_y = cv2.Sobel(gray_frame, cv2.CV_64F, 0, 1, ksize=5)  # Filtre de Sobel en Y\n",
        "        sobel_frame = cv2.magnitude(sobel_x, sobel_y)  # Combiner les deux directions\n",
        "\n",
        "        # Normaliser les valeurs entre 0 et 255\n",
        "        sobel_frame = cv2.normalize(sobel_frame, None, 0, 255, cv2.NORM_MINMAX)\n",
        "        sobel_frame = np.uint8(sobel_frame)  # Convertir en type uint8\n",
        "\n",
        "        # Enregistrer la frame modifiée\n",
        "        out.write(cv2.cvtColor(sobel_frame, cv2.COLOR_GRAY2BGR))  # Convertir en BGR pour l'enregistrement\n",
        "\n",
        "    # Libérer les ressources\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    print(f\"Le filtre de Sobel a été appliqué et la vidéo a été sauvegardée sous '{fichier_sortie}'.\")\n",
        "\n",
        "# Exemple pour appliquer le filtre de Sobel à une vidéo existante\n",
        "\n",
        "fichier_source = 'video_multimedia.mp4'  # Nom du fichier de la vidéo source\n",
        "fichier_sortie = 'video_sobel.mp4'  # Nom du fichier de la vidéo modifiée\n",
        "\n",
        "# Appliquer le filtre de Sobel\n",
        "appliquer_filtre_sobel(fichier_source, fichier_sortie)\n",
        "\n",
        "# Ce code va :\n",
        "# - Charger la vidéo 'video_multimedia.mp4'.\n",
        "# - Appliquer le filtre de Sobel à chaque frame.\n",
        "# - Sauvegarder la vidéo modifiée sous 'video_sobel.mp4'.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zj9hS6EhD3H0",
        "outputId": "6feb6855-ddc4-4909-9de7-34a9ea343d1b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le filtre de Sobel a été appliqué et la vidéo a été sauvegardée sous 'video_sobel.mp4'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5:"
      ],
      "metadata": {
        "id": "dDxvrVPmELMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def detecter_visages_dans_video(video_source, fichier_sortie):\n",
        "    \"\"\"\n",
        "    Détecte les visages dans une vidéo existante et les encadre.\n",
        "\n",
        "    Args:\n",
        "    - video_source (str) : Le fichier vidéo source.\n",
        "    - fichier_sortie (str) : Le fichier de sortie de la vidéo modifiée.\n",
        "    \"\"\"\n",
        "    # Charger le classificateur de visages pré-entrainé\n",
        "    visage_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "    # Charger la vidéo\n",
        "    cap = cv2.VideoCapture(video_source)\n",
        "\n",
        "    # Obtenir les propriétés de la vidéo\n",
        "    frame_width = int(cap.get(3))\n",
        "    frame_height = int(cap.get(4))\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "    # Définir le codec et l'objet VideoWriter pour enregistrer la vidéo modifiée\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    out = cv2.VideoWriter(fichier_sortie, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Convertir la frame en niveaux de gris pour la détection\n",
        "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Détecter les visages\n",
        "        visages = visage_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
        "\n",
        "        # Encadrer les visages détectés\n",
        "        for (x, y, w, h) in visages:\n",
        "            cv2.rectangle(frame, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "\n",
        "        # Enregistrer la frame modifiée\n",
        "        out.write(frame)\n",
        "\n",
        "    # Libérer les ressources\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    print(f\"La détection de visages a été effectuée et la vidéo a été sauvegardée sous '{fichier_sortie}'.\")\n",
        "\n",
        "# Exemple pour détecter des visages dans une vidéo existante\n",
        "\n",
        "fichier_source = 'video_multimedia.mp4'  # Nom du fichier de la vidéo source\n",
        "fichier_sortie = 'video_visages_detectes.mp4'  # Nom du fichier de la vidéo modifiée\n",
        "\n",
        "# Détecter les visages\n",
        "detecter_visages_dans_video(fichier_source, fichier_sortie)\n",
        "\n",
        "# Ce code va :\n",
        "# - Charger la vidéo 'video_multimedia.mp4'.\n",
        "# - Détecter les visages dans chaque frame et les encadrer.\n",
        "# - Sauvegarder la vidéo modifiée sous 'video_visages_detectes.mp4'.\n",
        "\n",
        "print(\"Ne peux pas être testé\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "Wc2kRouwEMjg",
        "outputId": "b7054fd9-6a80-4b03-b4ce-d46ed7c13538"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-ecdd90be59a7>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Détecter les visages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mdetecter_visages_dans_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfichier_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfichier_sortie\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Ce code va :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-ecdd90be59a7>\u001b[0m in \u001b[0;36mdetecter_visages_dans_video\u001b[0;34m(video_source, fichier_sortie)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Détecter les visages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mvisages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisage_cascade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetectMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray_frame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaleFactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminNeighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Encadrer les visages détectés\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}